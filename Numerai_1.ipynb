{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Numerai_1.ipynb","provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyPeBP1EfcHZbjPo6idj8Y4C"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nd4eqQN2caeE","executionInfo":{"status":"ok","timestamp":1631384412002,"user_tz":-540,"elapsed":7402,"user":{"displayName":"_whitecat_22","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivyFmvKSMdRRP_75JpaJPw1FBCcazPtuvLtSm2=s64","userId":"04862836480061657671"}},"outputId":"c22682e7-bcc2-4e83-c065-416ea8b419f8"},"source":["!pip install numerapi\n","!pip install utils\n","# !apt-get -qq install -y utils"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting numerapi\n","  Downloading numerapi-2.8.1-py3-none-any.whl (25 kB)\n","Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from numerapi) (1.1.5)\n","Requirement already satisfied: tqdm>=4.29.1 in /usr/local/lib/python3.7/dist-packages (from numerapi) (4.62.0)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from numerapi) (2018.9)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from numerapi) (2.23.0)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from numerapi) (2.8.2)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from numerapi) (7.1.2)\n","Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->numerapi) (1.19.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->numerapi) (1.15.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (3.0.4)\n","Installing collected packages: numerapi\n","Successfully installed numerapi-2.8.1\n","Collecting utils\n","  Downloading utils-1.0.1-py2.py3-none-any.whl (21 kB)\n","Installing collected packages: utils\n","Successfully installed utils-1.0.1\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":335},"id":"5XgGuCbZaA_B","executionInfo":{"status":"error","timestamp":1631384413037,"user_tz":-540,"elapsed":1046,"user":{"displayName":"_whitecat_22","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivyFmvKSMdRRP_75JpaJPw1FBCcazPtuvLtSm2=s64","userId":"04862836480061657671"}},"outputId":"0e1746c5-459b-4ede-ad26-dc9343d4dbb7"},"source":["import pandas as pd\n","from lightgbm import LGBMRegressor\n","import gc\n","from numerapi import NumerAPI\n","from utils import save_model, load_model, neutralize, get_biggest_change_features, validation_metrics, download_data, \\\n","    load_model_config, save_model_config, get_time_series_cross_val_splits"],"execution_count":2,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-51bc71790626>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumerapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNumerAPI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneutralize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_biggest_change_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_data\u001b[0m\u001b[0;34m,\u001b[0m     \u001b[0mload_model_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_model_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_time_series_cross_val_splits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mImportError\u001b[0m: cannot import name 'save_model' from 'utils' (/usr/local/lib/python3.7/dist-packages/utils/__init__.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","metadata":{"id":"aWKOBYfaaGCU","executionInfo":{"status":"aborted","timestamp":1631384413019,"user_tz":-540,"elapsed":11,"user":{"displayName":"_whitecat_22","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivyFmvKSMdRRP_75JpaJPw1FBCcazPtuvLtSm2=s64","userId":"04862836480061657671"}}},"source":["EXAMPLE_PREDS_COL = \"example_preds\"\n","TARGET_COL = \"target\"\n","ERA_COL = \"era\"\n","# params we'll use to train all of our models.\n","# Ideal params would be more like 20000, 0.001, 6, 2**6, 0.1, but this is slow enough as it is\n","model_params = {\"n_estimators\": 2000,\n","                \"learning_rate\": 0.01,\n","                \"max_depth\": 5,\n","                \"num_leaves\": 2 ** 5,\n","                \"colsample_bytree\": 0.1}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xk-zEQyCaK3N","executionInfo":{"status":"aborted","timestamp":1631384413021,"user_tz":-540,"elapsed":13,"user":{"displayName":"_whitecat_22","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivyFmvKSMdRRP_75JpaJPw1FBCcazPtuvLtSm2=s64","userId":"04862836480061657671"}}},"source":["# the amount of downsampling we'll use to speed up cross validation and full train.\n","# a value of 1 means no downsampling\n","# a value of 10 means use every 10th row\n","downsample_cross_val = 20\n","downsample_full_train = 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kZtha3iLaWjo","executionInfo":{"status":"aborted","timestamp":1631384413023,"user_tz":-540,"elapsed":15,"user":{"displayName":"_whitecat_22","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivyFmvKSMdRRP_75JpaJPw1FBCcazPtuvLtSm2=s64","userId":"04862836480061657671"}}},"source":["# if model_selection_loop=True get OOS performance for training_data\n","# and use that to select best model\n","# if model_selection_loop=False, just predict on tournament data using existing models and model config\n","model_selection_loop = True\n","model_config_name = \"advanced_example_model\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"81Nb7SGIaQXh","executionInfo":{"status":"aborted","timestamp":1631384413024,"user_tz":-540,"elapsed":16,"user":{"displayName":"_whitecat_22","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivyFmvKSMdRRP_75JpaJPw1FBCcazPtuvLtSm2=s64","userId":"04862836480061657671"}}},"source":["napi = NumerAPI()\n","\n","current_round = napi.get_current_round(tournament=8)  # tournament 8 is the primary Numerai Tournament"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UqUZDfvHaelG","executionInfo":{"status":"aborted","timestamp":1631384413026,"user_tz":-540,"elapsed":17,"user":{"displayName":"_whitecat_22","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivyFmvKSMdRRP_75JpaJPw1FBCcazPtuvLtSm2=s64","userId":"04862836480061657671"}}},"source":["print(\"Entering model selection loop.  This may take awhile.\")\n","if model_selection_loop:\n","    model_config = {}\n","    print('downloading training_data')\n","    download_data(napi, 'numerai_training_data.parquet', 'numerai_training_data.parquet', round=current_round)\n","\n","    print(\"reading training data from local file\")\n","    training_data = pd.read_parquet('numerai_training_data.parquet')\n","\n","    # keep track of some prediction columns\n","    ensemble_cols = set()\n","    pred_cols = set()\n","\n","    # pick some targets to use\n","    possible_targets = [c for c in training_data.columns if c.startswith(\"target_\")]\n","    # randomly pick a handful of targets\n","    # this can be vastly improved\n","    targets = [\"target\", \"target_nomi_60\", \"target_jerome_20\"]\n","\n","    # all the possible features to train on\n","    feature_cols = [c for c in training_data if c.startswith(\"feature_\")]\n","\n","    \"\"\" do cross val to get out of sample training preds\"\"\"\n","    cv = 3\n","    train_test_zip = get_time_series_cross_val_splits(training_data, cv=cv, embargo=12)\n","    # get out of sample training preds via embargoed time series cross validation\n","    # optionally downsample training data to speed up this section.\n","    print(\"entering time series cross validation loop\")\n","    for split, train_test_split in enumerate(train_test_zip):\n","        gc.collect()\n","        print(f\"doing split {split+1} out of {cv}\")\n","        train_split, test_split = train_test_split\n","        train_split_index = training_data[ERA_COL].isin(train_split)\n","        test_split_index = training_data[ERA_COL].isin(test_split)\n","        downsampled_train_split_index = train_split_index[train_split_index].index[::downsample_cross_val]\n","\n","        # getting the per era correlation of each feature vs the primary target across the training split\n","        print(\"getting feature correlations over time and identifying riskiest features\")\n","        all_feature_corrs_split = training_data.loc[downsampled_train_split_index, :].groupby(ERA_COL).apply(\n","            lambda d: d[feature_cols].corrwith(d[TARGET_COL]))\n","        # find the riskiest features by comparing their correlation vs the target in half 1 and half 2 of training data\n","        # there are probably more clever ways to do this\n","        riskiest_features_split = get_biggest_change_features(all_feature_corrs_split, 50)\n","\n","        print(f\"entering model training loop for split {split+1}\")\n","        for target in targets:\n","            model_name = f\"model_{target}\"\n","            print(f\"model: {model_name}\")\n","\n","            # train a model on the training split (and save it for future use)\n","            split_model_name = f\"model_{target}_split{split+1}cv{cv}downsample{downsample_cross_val}\"\n","            split_model = load_model(split_model_name)\n","            if not split_model:\n","                print(f\"training model: {model_name}\")\n","                split_model = LGBMRegressor(**model_params)\n","                split_model.fit(training_data.loc[downsampled_train_split_index, feature_cols],\n","                                training_data.loc[downsampled_train_split_index,\n","                                                  [target]])\n","                save_model(split_model, split_model_name)\n","            # now we can predict on the test part of the split\n","            model_expected_features = split_model.booster_.feature_name()\n","            if set(model_expected_features) != set(feature_cols):\n","                print(f\"New features are available! Might want to retrain model {split_model_name}.\")\n","            print(f\"predicting {model_name}\")\n","            training_data.loc[test_split_index, f\"preds_{model_name}\"] = \\\n","                split_model.predict(training_data.loc[test_split_index, model_expected_features])\n","\n","            # do neutralization\n","            print(\"doing neutralization to riskiest features\")\n","            training_data.loc[test_split_index, f\"preds_{model_name}_neutral_riskiest_50\"] = neutralize(\n","                df=training_data.loc[test_split_index, :],\n","                columns=[f\"preds_{model_name}\"],\n","                neutralizers=riskiest_features_split,\n","                proportion=1.0,\n","                normalize=True,\n","                era_col=ERA_COL)[f\"preds_{model_name}\"]\n","\n","            # remember that we made all of these different pred columns\n","            pred_cols.add(f\"preds_{model_name}\")\n","            pred_cols.add(f\"preds_{model_name}_neutral_riskiest_50\")\n","\n","        print(\"creating ensembles\")\n","        # ranking per era for all of our pred cols so we can combine safely on the same scales\n","        training_data[list(pred_cols)] = training_data.groupby(ERA_COL).apply(\n","            lambda d: d[list(pred_cols)].rank(pct=True))\n","        # do ensembles\n","        training_data[\"ensemble_neutral_riskiest_50\"] = sum(\n","            [training_data[pred_col] for pred_col in pred_cols if pred_col.endswith(\"neutral_riskiest_50\")]).rank(\n","            pct=True)\n","        training_data[\"ensemble_not_neutral\"] = sum(\n","            [training_data[pred_col] for pred_col in pred_cols if \"neutral\" not in pred_col]).rank(pct=True)\n","        training_data[\"ensemble_all\"] = sum([training_data[pred_col] for pred_col in pred_cols]).rank(pct=True)\n","\n","        ensemble_cols.add(\"ensemble_neutral_riskiest_50\")\n","        ensemble_cols.add(\"ensemble_not_neutral\")\n","        ensemble_cols.add(\"ensemble_all\")\n","\n","    \"\"\" Now get some stats and pick our favorite model\"\"\"\n","    print(\"gathering validation metrics for out of sample training results\")\n","    all_model_cols = list(pred_cols) + list(ensemble_cols)\n","    # use example_col preds_model_target as an estimates since no example preds provided for training\n","    # fast_mode=True so that we skip some of the stats that are slower to calculate\n","    training_stats = validation_metrics(training_data, all_model_cols, example_col=\"preds_model_target\",\n","                                        fast_mode=True)\n","    print(training_stats[[\"mean\", \"sharpe\"]].sort_values(by=\"sharpe\", ascending=False).to_markdown())\n","\n","    # pick the model that has the highest correlation sharpe\n","    best_pred_col = training_stats.sort_values(by=\"sharpe\", ascending=False).head(1).index[0]\n","    print(f\"selecting model {best_pred_col} as our highest sharpe model in validation\")\n","\n","    \"\"\" Now do a full train\"\"\"\n","    print(\"entering full training section\")\n","    # getting the per era correlation of each feature vs the target across all of training data\n","    print(\"getting feature correlations with target and identifying riskiest features\")\n","    all_feature_corrs = training_data.groupby(ERA_COL).apply(\n","        lambda d: d[feature_cols].corrwith(d[TARGET_COL]))\n","    # find the riskiest features by comparing their correlation vs the target in half 1 and half 2 of training data\n","    riskiest_features = get_biggest_change_features(all_feature_corrs, 50)\n","\n","    for target in targets:\n","        gc.collect()\n","        model_name = f\"model_{target}_downsample{downsample_full_train}\"\n","        model = load_model(model_name)\n","        if not model:\n","            print(f\"training {model_name}\")\n","            model = LGBMRegressor(**model_params)\n","            # train on all of train, predict on val, predict on tournament\n","            model.fit(training_data.iloc[::downsample_full_train].loc[:, feature_cols],\n","                      training_data.iloc[::downsample_full_train][target])\n","            save_model(model, model_name)\n","        gc.collect()\n","\n","    model_config[\"feature_cols\"] = feature_cols\n","    model_config[\"targets\"] = targets\n","    model_config[\"best_pred_col\"] = best_pred_col\n","    model_config[\"riskiest_features\"] = riskiest_features\n","    print(f\"saving model config for {model_config_name}\")\n","    save_model_config(model_config, model_config_name)\n","else:\n","    # load model config from previous model selection loop\n","    print(f\"loading model config for {model_config_name}\")\n","    model_config = load_model_config(model_config_name)\n","    feature_cols = model_config[\"feature_cols\"]\n","    targets = model_config[\"targets\"]\n","    best_pred_col = model_config[\"best_pred_col\"]\n","    riskiest_features = model_config[\"riskiest_features\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6xuUpHsiannO","executionInfo":{"status":"aborted","timestamp":1631384413027,"user_tz":-540,"elapsed":18,"user":{"displayName":"_whitecat_22","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivyFmvKSMdRRP_75JpaJPw1FBCcazPtuvLtSm2=s64","userId":"04862836480061657671"}}},"source":["\"\"\" Things that we always do even if we've already trained \"\"\"\n","gc.collect()\n","print(\"downloading tournament_data\")\n","download_data(napi, 'numerai_tournament_data.parquet', f'numerai_tournament_data_{current_round}.parquet',\n","              round=current_round)\n","print(\"downloading validation_data\")\n","download_data(napi, 'numerai_validation_data.parquet', 'numerai_validation_data.parquet', round=current_round)\n","print(\"downloading example_predictions\")\n","download_data(napi, 'example_predictions.parquet', f'example_predictions_{current_round}.parquet',\n","              round=current_round)\n","print(\"downloading example_validation_predictions\")\n","download_data(napi, 'example_validation_predictions.parquet', f'example_validation_predictions.parquet',\n","              round=current_round)\n","\n","print(\"reading tournament_data\")\n","tournament_data = pd.read_parquet(f'numerai_tournament_data_{current_round}.parquet')\n","print(\"reading validation_data\")\n","validation_data = pd.read_parquet('numerai_validation_data.parquet')\n","print(\"reading example_predictions\")\n","example_preds = pd.read_parquet(f'example_predictions_{current_round}.parquet')\n","print(\"reading example_validaton_predictions\")\n","validation_example_preds = pd.read_parquet('example_validation_predictions.parquet')\n","# set the example predictions\n","validation_data[EXAMPLE_PREDS_COL] = validation_example_preds[\"prediction\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2e9jfQ8MazY9","executionInfo":{"status":"aborted","timestamp":1631384413028,"user_tz":-540,"elapsed":19,"user":{"displayName":"_whitecat_22","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivyFmvKSMdRRP_75JpaJPw1FBCcazPtuvLtSm2=s64","userId":"04862836480061657671"}}},"source":["# check for nans and fill nans\n","print(\"checking for nans in the tournament data\")\n","if tournament_data.loc[tournament_data[\"data_type\"] == \"live\", feature_cols].isna().sum().sum():\n","    cols_w_nan = tournament_data.loc[tournament_data[\"data_type\"] == \"live\", feature_cols].isna().sum()\n","    total_rows = tournament_data[tournament_data[\"data_type\"] == \"live\"]\n","    print(f\"Number of nans per column this week: {cols_w_nan[cols_w_nan > 0]}\")\n","    print(f\"out of {total_rows} total rows\")\n","    print(f\"filling nans with 0.5\")\n","    tournament_data.loc[:, feature_cols].fillna(0.5, inplace=True)\n","else:\n","    print(\"No nans in the features this week!\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DwCEZpNEa1Sh","executionInfo":{"status":"aborted","timestamp":1631384413029,"user_tz":-540,"elapsed":20,"user":{"displayName":"_whitecat_22","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivyFmvKSMdRRP_75JpaJPw1FBCcazPtuvLtSm2=s64","userId":"04862836480061657671"}}},"source":["pred_cols = set()\n","ensemble_cols = set()\n","for target in targets:\n","    gc.collect()\n","    model_name = f\"model_{target}_downsample{downsample_full_train}\"\n","    print(f\"loading {model_name}\")\n","    model = load_model(model_name)\n","    if not model:\n","        raise ValueError(f\"{model_name} is not trained yet!\")\n","\n","    model_expected_features = model.booster_.feature_name()\n","    if set(model_expected_features) != set(feature_cols):\n","        print(f\"New features are available! Might want to retrain model {model_name}.\")\n","    print(f\"predicting tournament and validation for {model_name}\")\n","    validation_data.loc[:, f\"preds_{model_name}\"] = model.predict(validation_data.loc[:, model_expected_features])\n","    tournament_data.loc[:, f\"preds_{model_name}\"] = model.predict(tournament_data.loc[:, model_expected_features])\n","\n","    # do different neutralizations\n","    # neutralize our predictions to the riskiest features only\n","    print(\"neutralizing to riskiest_50 for validation and tournament\")\n","    validation_data[f\"preds_{model_name}_neutral_riskiest_50\"] = neutralize(df=validation_data,\n","                                                                            columns=[f\"preds_{model_name}\"],\n","                                                                            neutralizers=riskiest_features,\n","                                                                            proportion=1.0,\n","                                                                            normalize=True,\n","                                                                            era_col=ERA_COL)[f\"preds_{model_name}\"]\n","    tournament_data[f\"preds_{model_name}_neutral_riskiest_50\"] = neutralize(df=tournament_data,\n","                                                                            columns=[f\"preds_{model_name}\"],\n","                                                                            neutralizers=riskiest_features,\n","                                                                            proportion=1.0,\n","                                                                            normalize=True,\n","                                                                            era_col=ERA_COL)[f\"preds_{model_name}\"]\n","\n","    pred_cols.add(f\"preds_{model_name}\")\n","    pred_cols.add(f\"preds_{model_name}_neutral_riskiest_50\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8I1C0IkZa4Q_","executionInfo":{"status":"aborted","timestamp":1631384413030,"user_tz":-540,"elapsed":21,"user":{"displayName":"_whitecat_22","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivyFmvKSMdRRP_75JpaJPw1FBCcazPtuvLtSm2=s64","userId":"04862836480061657671"}}},"source":["# rank per era for each prediction column so that we can combine safely\n","validation_data[list(pred_cols)] = validation_data.groupby(ERA_COL).apply(lambda d: d[list(pred_cols)].rank(pct=True))\n","tournament_data[list(pred_cols)] = tournament_data.groupby(ERA_COL).apply(lambda d: d[list(pred_cols)].rank(pct=True))\n","# make ensembles for val and tournament\n","print('creating ensembles for tournament and validation')\n","validation_data[\"ensemble_neutral_riskiest_50\"] = sum(\n","    [validation_data[pred_col] for pred_col in pred_cols if pred_col.endswith(\"neutral_riskiest_50\")]).rank(\n","    pct=True)\n","tournament_data[\"ensemble_neutral_riskiest_50\"] = sum(\n","    [tournament_data[pred_col] for pred_col in pred_cols if pred_col.endswith(\"neutral_riskiest_50\")]).rank(\n","    pct=True)\n","ensemble_cols.add(\"ensemble_neutral_riskiest_50\")\n","\n","validation_data[\"ensemble_not_neutral\"] = sum(\n","    [validation_data[pred_col] for pred_col in pred_cols if \"neutral\" not in pred_col]).rank(pct=True)\n","tournament_data[\"ensemble_not_neutral\"] = sum(\n","    [tournament_data[pred_col] for pred_col in pred_cols if \"neutral\" not in pred_col]).rank(pct=True)\n","ensemble_cols.add(\"ensemble_not_neutral\")\n","\n","validation_data[\"ensemble_all\"] = sum([validation_data[pred_col] for pred_col in pred_cols]).rank(pct=True)\n","tournament_data[\"ensemble_all\"] = sum([tournament_data[pred_col] for pred_col in pred_cols]).rank(pct=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jUoLynFXa8NV","executionInfo":{"status":"aborted","timestamp":1631384413032,"user_tz":-540,"elapsed":22,"user":{"displayName":"_whitecat_22","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivyFmvKSMdRRP_75JpaJPw1FBCcazPtuvLtSm2=s64","userId":"04862836480061657671"}}},"source":["ensemble_cols.add(\"ensemble_neutral_riskiest_50\")\n","ensemble_cols.add(\"ensemble_not_neutral\")\n","ensemble_cols.add(\"ensemble_all\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wMgwICX_a-r7","executionInfo":{"status":"aborted","timestamp":1631384413033,"user_tz":-540,"elapsed":23,"user":{"displayName":"_whitecat_22","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivyFmvKSMdRRP_75JpaJPw1FBCcazPtuvLtSm2=s64","userId":"04862836480061657671"}}},"source":["gc.collect()\n","print(\"getting final validation stats\")\n","# get our final validation stats for our chosen model\n","validation_stats = validation_metrics(validation_data, [best_pred_col], example_col=EXAMPLE_PREDS_COL,\n","                                      fast_mode=False)\n","print(validation_stats.to_markdown())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pguyO3ekbEcD","executionInfo":{"status":"aborted","timestamp":1631384413035,"user_tz":-540,"elapsed":25,"user":{"displayName":"_whitecat_22","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivyFmvKSMdRRP_75JpaJPw1FBCcazPtuvLtSm2=s64","userId":"04862836480061657671"}}},"source":["# rename best model to prediction and rank from 0 to 1 to meet diagnostic/submission file requirements\n","validation_data[\"prediction\"] = validation_data[best_pred_col].rank(pct=True)\n","tournament_data[\"prediction\"] = tournament_data[best_pred_col].rank(pct=True)\n","validation_data[\"prediction\"].to_csv(f\"prediction_files/validation_predictions_{current_round}.csv\", index=True)\n","tournament_data[\"prediction\"].to_csv(f\"prediction_files/tournament_predictions_{current_round}.csv\", index=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YU_r85J5bGkE","executionInfo":{"status":"aborted","timestamp":1631384413036,"user_tz":-540,"elapsed":26,"user":{"displayName":"_whitecat_22","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivyFmvKSMdRRP_75JpaJPw1FBCcazPtuvLtSm2=s64","userId":"04862836480061657671"}}},"source":["example_public_id = \"DP3ULI36EUVPVRLVKXONO6PVKFSB27C5\"\n","example_secret_key = \"AUYB4PKXJ5RWPS4546GLE4ZGNUPOXIA6XUMJNL2XTPDSO6QPG26COOJSK3UWEI22\"\n","napi = numerapi.NumerAPI(example_public_id, example_secret_key)\n","# upload predictions\n","model_id = napi.get_models()['5d445fe8-3cfc-4fea-a9c4-900baa246e10']\n","napi.upload_predictions(f\"tournament_predictions_{current_round}.csv\", model_id=model_id)"],"execution_count":null,"outputs":[]}]}